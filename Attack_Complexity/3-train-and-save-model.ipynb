{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93310, 1)\n",
      "(23328, 1)\n",
      "(93310, 1)\n",
      "(23328, 1)\n",
      "cvssv3_attack_complexity\n",
      "HIGH    0.501072\n",
      "LOW     0.498928\n",
      "Name: count, dtype: float64\n",
      "cvssv3_attack_complexity\n",
      "LOW     0.504287\n",
      "HIGH    0.495713\n",
      "Name: count, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93310 entries, 0 to 93309\n",
      "Data columns (total 1 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   english_description  93310 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 729.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93310 entries, 0 to 93309\n",
      "Data columns (total 1 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   cvssv3_attack_complexity  93310 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 729.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_train = pd.read_csv('./cvss_2022_2024_X_train.csv')\n",
    "y_train = pd.read_csv('./cvss_2022_2024_y_train.csv')\n",
    "\n",
    "X_test = pd.read_csv('./cvss_2022_2024_X_test.csv')\n",
    "y_test = pd.read_csv('./cvss_2022_2024_y_test.csv')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_train['cvssv3_attack_complexity'].value_counts(dropna=False) / y_train.shape[0])\n",
    "print(y_test['cvssv3_attack_complexity'].value_counts(dropna=False) / y_test.shape[0])\n",
    "print(X_train.info())\n",
    "print(y_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HIGH' 'LOW']\n",
      "2\n",
      "0     LOW\n",
      "1     LOW\n",
      "2     LOW\n",
      "3    HIGH\n",
      "4     LOW\n",
      "5    HIGH\n",
      "6    HIGH\n",
      "7    HIGH\n",
      "8     LOW\n",
      "9     LOW\n",
      "Name: cvssv3_attack_complexity, dtype: object [1 1 1 0 1 0 0 0 1 1]\n",
      "93310 93310 23328 23328\n"
     ]
    }
   ],
   "source": [
    "label_column_name = \"cvssv3_attack_complexity\"\n",
    "train_labels = y_train.loc[:, label_column_name]\n",
    "test_labels = y_test.loc[:, label_column_name]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "print(le.classes_)\n",
    "\n",
    "with open(\"../labels/attack_complexity_label.txt\", \"wb\") as f:\n",
    "    pickle.dump(le.classes_, f)\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "encoded_train_labels = le.transform(train_labels)\n",
    "encoded_test_labels = le.transform(test_labels)\n",
    "\n",
    "print(train_labels[:10], encoded_train_labels[:10])\n",
    "print(len(X_train), len(train_labels), len(X_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CVEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, encodings, labels, encoded_labels):\n",
    "        self.texts = X.loc[:,\"english_description\"].tolist()\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "        self.encoded_labels = encoded_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['text_labels'] = self.labels[idx]\n",
    "        item['encoded_labels'] = torch.tensor(self.encoded_labels[idx])\n",
    "        item['vulnerability_description'] = self.texts[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CVEDataset(X_train, train_encodings, train_labels, encoded_train_labels)\n",
    "test_dataset = CVEDataset(X_test, test_encodings, test_labels, encoded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1000,  1996,  7209,  5587,  5644,  2005,  1059,  2361,  3676,\n",
       "          5484,  2100, 13354,  2378,  2005,  2773, 20110,  2003,  8211,  2000,\n",
       "          8250,  2892,  1011,  2609,  5896,  2075,  3081,  1996, 13354,  2378,\n",
       "          1005,  1055,  7209,  1035,  7037,  1035,  3609,  2460, 16044,  1999,\n",
       "          2035,  4617,  2039,  2000,  1010,  1998,  2164,  1010,  1017,  1012,\n",
       "          2539,  1012,  2322,  2349,  2000, 13990,  7953,  2624, 25090,  9276,\n",
       "          1998,  6434, 13002,  2006,  5310,  8127, 12332,  1012,  2023,  3084,\n",
       "          2009,  2825,  2005, 14469,  4383, 17857,  1010,  2007, 12130,  1011,\n",
       "          2504,  3229,  1998,  2682,  1010,  2000,  1999, 20614, 15275,  4773,\n",
       "         14546,  1999,  5530,  2008,  2097, 15389,  7188,  1037,  5310,  3229,\n",
       "          2229,  2019, 19737,  3931,  1012,  1000,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'text_labels': 'LOW',\n",
       " 'encoded_labels': tensor(1),\n",
       " 'vulnerability_description': '\"The Ultimate Addons for WPBakery plugin for WordPress is vulnerable to Stored Cross-Site Scripting via the plugin\\'s ultimate_dual_color shortcode in all versions up to, and including, 3.19.20 due to insufficient input sanitization and output escaping on user supplied attributes. This makes it possible for authenticated attackers, with contributor-level access and above, to inject arbitrary web scripts in pages that will execute whenever a user accesses an injected page.\"'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-small', num_labels=NUM_CLASSES)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight tensor([[-0.0069, -0.0152,  0.0380,  ..., -0.0173,  0.0061,  0.0151],\n",
      "        [ 0.0100, -0.0112, -0.0140,  ..., -0.0047,  0.0014, -0.0099]])\n",
      "classifier.bias tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.6144748267311335, Training Accuracy = 0.6621583967420427\n",
      "Epoch: 1, Training Loss: 0.5785503843438522, Training Accuracy = 0.6979852105883614\n",
      "Epoch: 2, Training Loss: 0.5693199700039818, Training Accuracy = 0.705454935162362\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # print(name, param.data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.1557001861908215, Training Accuracy = 0.9386668095595327\n",
      "Epoch: 1, Training Loss: 0.05364338152971983, Training Accuracy = 0.9830028935805379\n",
      "Epoch: 2, Training Loss: 0.039294798068496416, Training Accuracy = 0.9876969242310578\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../models/bert-small-vulnerability_attack_complexity-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.040389585217806084, Test Accuracy = 0.9889831961591221\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "num_correct = 0 \n",
    "num_examples = 0\n",
    "test_loss = 0\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['encoded_labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    test_loss += loss.data.item() * input_ids.size(0)\n",
    "    correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "    num_correct += torch.sum(correct).item()\n",
    "    num_examples += correct.shape[0]\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "print('Test Loss: {}, Test Accuracy = {}'.format(test_loss, num_correct / num_examples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
