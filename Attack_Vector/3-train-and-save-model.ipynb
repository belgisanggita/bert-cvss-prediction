{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142192, 1)\n",
      "(35548, 1)\n",
      "(142192, 1)\n",
      "(35548, 1)\n",
      "cvssv3_attack_vector\n",
      "ADJACENT_NETWORK    0.250225\n",
      "NETWORK             0.250077\n",
      "PHYSICAL            0.249887\n",
      "LOCAL               0.249810\n",
      "Name: count, dtype: float64\n",
      "cvssv3_attack_vector\n",
      "LOCAL               0.250760\n",
      "PHYSICAL            0.250450\n",
      "NETWORK             0.249691\n",
      "ADJACENT_NETWORK    0.249100\n",
      "Name: count, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142192 entries, 0 to 142191\n",
      "Data columns (total 1 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   english_description  142192 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142192 entries, 0 to 142191\n",
      "Data columns (total 1 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   cvssv3_attack_vector  142192 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_train = pd.read_csv('./cvss_2022_2024_X_train-attack-vector.csv')\n",
    "y_train = pd.read_csv('./cvss_2022_2024_y_train-attack-vector.csv')\n",
    "\n",
    "X_test = pd.read_csv('./cvss_2022_2024_X_test-attack-vector.csv')\n",
    "y_test = pd.read_csv('./cvss_2022_2024_y_test-attack-vector.csv')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_train['cvssv3_attack_vector'].value_counts(dropna=False) / y_train.shape[0])\n",
    "print(y_test['cvssv3_attack_vector'].value_counts(dropna=False) / y_test.shape[0])\n",
    "print(X_train.info())\n",
    "print(y_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJACENT_NETWORK' 'LOCAL' 'NETWORK' 'PHYSICAL']\n",
      "4\n",
      "0               LOCAL\n",
      "1    ADJACENT_NETWORK\n",
      "2               LOCAL\n",
      "3            PHYSICAL\n",
      "4            PHYSICAL\n",
      "5               LOCAL\n",
      "6             NETWORK\n",
      "7    ADJACENT_NETWORK\n",
      "8            PHYSICAL\n",
      "9               LOCAL\n",
      "Name: cvssv3_attack_vector, dtype: object [1 0 1 3 3 1 2 0 3 1]\n",
      "142192 142192 35548 35548\n"
     ]
    }
   ],
   "source": [
    "label_column_name = \"cvssv3_attack_vector\"\n",
    "train_labels = y_train.loc[:, label_column_name]\n",
    "test_labels = y_test.loc[:, label_column_name]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "print(le.classes_)\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "encoded_train_labels = le.transform(train_labels)\n",
    "encoded_test_labels = le.transform(test_labels)\n",
    "\n",
    "print(train_labels[:10], encoded_train_labels[:10])\n",
    "print(len(X_train), len(train_labels), len(X_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CVEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, encodings, labels, encoded_labels):\n",
    "        self.texts = X.loc[:,\"english_description\"].tolist()\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "        self.encoded_labels = encoded_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['text_labels'] = self.labels[idx]\n",
    "        item['encoded_labels'] = torch.tensor(self.encoded_labels[idx])\n",
    "        item['vulnerability_description'] = self.texts[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CVEDataset(X_train, train_encodings, train_labels, encoded_train_labels)\n",
    "test_dataset = CVEDataset(X_test, test_encodings, test_labels, encoded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1000, 18106,  9353,  3217, 14479,  8068,  4617,  2570,  1012,\n",
       "         25604,  1012,  2297,  2475,  1006,  1998,  3041,  1007,  1010,  2322,\n",
       "          1012,  4002,  2629,  1012, 19988, 22022,  1006,  1998,  3041,  1007,\n",
       "          1998,  2459,  1012,  5890,  2475,  1012, 22060, 24594,  1006,  1998,\n",
       "          3041,  1007,  2024,  5360,  2011,  2019,  2041,  1011,  1997,  1011,\n",
       "         19202,  3191, 18130,  2043, 11968,  7741,  1037, 19275,  5371,  1010,\n",
       "          2029,  2071,  2765,  1999,  1037,  3191,  2627,  1996,  2203,  1997,\n",
       "          2019, 11095,  3638,  3252,  1012,  2019, 17346,  2071, 21155,  2023,\n",
       "         18130,  2000, 15389,  3642,  1999,  1996,  6123,  1997,  1996,  2783,\n",
       "          5310,  1012, 14427,  1997,  2023,  3277,  5942,  5310,  8290,  1999,\n",
       "          2008,  1037,  6778,  2442,  2330,  1037, 24391,  5371,  1012,  1000,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'text_labels': 'LOCAL',\n",
       " 'encoded_labels': tensor(1),\n",
       " 'vulnerability_description': '\"Adobe Acrobat Reader versions 22.001.20142 (and earlier), 20.005.30334 (and earlier) and 17.012.30229 (and earlier) are affected by an out-of-bounds read vulnerability when parsing a crafted file, which could result in a read past the end of an allocated memory structure. An attacker could leverage this vulnerability to execute code in the context of the current user. Exploitation of this issue requires user interaction in that a victim must open a malicious file.\"'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-small', num_labels=NUM_CLASSES)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight tensor([[-0.0314, -0.0097, -0.0061,  ...,  0.0090, -0.0013,  0.0197],\n",
      "        [-0.0002, -0.0158,  0.0058,  ...,  0.0057, -0.0043,  0.0264],\n",
      "        [ 0.0054, -0.0179,  0.0141,  ...,  0.0263, -0.0139, -0.0144],\n",
      "        [-0.0172,  0.0377,  0.0102,  ...,  0.0009,  0.0093,  0.0172]])\n",
      "classifier.bias tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.1583875328439877, Training Accuracy = 0.5215061325531676\n",
      "Epoch: 1, Training Loss: 1.0265454620817078, Training Accuracy = 0.5901808821874648\n",
      "Epoch: 2, Training Loss: 0.9861498570246356, Training Accuracy = 0.6073900078766737\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # print(name, param.data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.17524454202803993, Training Accuracy = 0.9388854506582649\n",
      "Epoch: 1, Training Loss: 0.08279674382779084, Training Accuracy = 0.9726144930797794\n",
      "Epoch: 2, Training Loss: 0.06153090472359961, Training Accuracy = 0.9803575447282548\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../models/bert-small-vulnerability_attack_vector-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06995268233049547, Test Accuracy = 0.9790986834702374\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "num_correct = 0 \n",
    "num_examples = 0\n",
    "test_loss = 0\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['encoded_labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    test_loss += loss.data.item() * input_ids.size(0)\n",
    "    correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "    num_correct += torch.sum(correct).item()\n",
    "    num_examples += correct.shape[0]\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "print('Test Loss: {}, Test Accuracy = {}'.format(test_loss, num_correct / num_examples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
