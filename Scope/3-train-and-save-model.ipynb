{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77334, 1)\n",
      "(19334, 1)\n",
      "(77334, 1)\n",
      "(19334, 1)\n",
      "cvssv3_scope\n",
      "CHANGED      0.500194\n",
      "UNCHANGED    0.499806\n",
      "Name: count, dtype: float64\n",
      "cvssv3_scope\n",
      "UNCHANGED    0.500776\n",
      "CHANGED      0.499224\n",
      "Name: count, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77334 entries, 0 to 77333\n",
      "Data columns (total 1 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   english_description  77334 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 604.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77334 entries, 0 to 77333\n",
      "Data columns (total 1 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   cvssv3_scope  77334 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 604.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_train = pd.read_csv('./cvss_2022_2024_X_train.csv')\n",
    "y_train = pd.read_csv('./cvss_2022_2024_y_train.csv')\n",
    "\n",
    "X_test = pd.read_csv('./cvss_2022_2024_X_test.csv')\n",
    "y_test = pd.read_csv('./cvss_2022_2024_y_test.csv')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_train['cvssv3_scope'].value_counts(dropna=False) / y_train.shape[0])\n",
    "print(y_test['cvssv3_scope'].value_counts(dropna=False) / y_test.shape[0])\n",
    "print(X_train.info())\n",
    "print(y_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHANGED' 'UNCHANGED']\n",
      "2\n",
      "0      CHANGED\n",
      "1    UNCHANGED\n",
      "2    UNCHANGED\n",
      "3    UNCHANGED\n",
      "4      CHANGED\n",
      "5      CHANGED\n",
      "6    UNCHANGED\n",
      "7      CHANGED\n",
      "8    UNCHANGED\n",
      "9    UNCHANGED\n",
      "Name: cvssv3_scope, dtype: object [0 1 1 1 0 0 1 0 1 1]\n",
      "77334 77334 19334 19334\n"
     ]
    }
   ],
   "source": [
    "label_column_name = \"cvssv3_scope\"\n",
    "train_labels = y_train.loc[:, label_column_name]\n",
    "test_labels = y_test.loc[:, label_column_name]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "print(le.classes_)\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "encoded_train_labels = le.transform(train_labels)\n",
    "encoded_test_labels = le.transform(test_labels)\n",
    "\n",
    "print(train_labels[:10], encoded_train_labels[:10])\n",
    "print(len(X_train), len(train_labels), len(X_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CVEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, encodings, labels, encoded_labels):\n",
    "        self.texts = X.loc[:,\"english_description\"].tolist()\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "        self.encoded_labels = encoded_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['text_labels'] = self.labels[idx]\n",
    "        item['encoded_labels'] = torch.tensor(self.encoded_labels[idx])\n",
    "        item['vulnerability_description'] = self.texts[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CVEDataset(X_train, train_encodings, train_labels, encoded_train_labels)\n",
    "test_dataset = CVEDataset(X_test, test_encodings, test_labels, encoded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1000, 24156,  8699,  3989,  1997,  7953,  2076,  4773,  3931,\n",
       "          4245, 18130,  1999,  1000, 10651,  1997,  3167,  4751,  1000,  2433,\n",
       "          1999,  9530, 26807,  9686,  2361, 17850,  2968,  4473,  8250,  1060,\n",
       "          4757,  2886,  1012,  2019, 17346,  2453,  1999, 20614,  1037,  5896,\n",
       "          2000,  2022,  2448,  1999,  5310,  1005,  1055, 16602,  1012,  2044,\n",
       "          3674,  4740,  2000,  3967,  1996, 21431,  2057,  2106,  2025,  4374,\n",
       "          2151,  3437,  1012,  1996,  2424,  2121,  3024,  1996,  2592,  2008,\n",
       "          2023,  3277, 13531,  9686,  2361, 17850,  2968,  4617,  2077,  1020,\n",
       "          1012,  1020,  1012,  1000,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'text_labels': 'CHANGED',\n",
       " 'encoded_labels': tensor(0),\n",
       " 'vulnerability_description': '\"Improper Neutralization of Input During Web Page Generation vulnerability in \"Update of Personal Details\" form in ConnX ESP HR Management allows Stored XSS attack.\\xa0An attacker might inject a script to be run in user\\'s browser.\\xa0After multiple attempts to contact the vendor we did not receive any answer. The finder provided the information that\\xa0this issue affects ESP HR Management versions before 6.6.\"'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-small', num_labels=NUM_CLASSES)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight tensor([[ 0.0017,  0.0148, -0.0090,  ..., -0.0098, -0.0107, -0.0141],\n",
      "        [-0.0193, -0.0306,  0.0141,  ..., -0.0004, -0.0072, -0.0121]])\n",
      "classifier.bias tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.5855226599778907, Training Accuracy = 0.7052008172343341\n",
      "Epoch: 1, Training Loss: 0.5086474053539143, Training Accuracy = 0.7641011715416246\n",
      "Epoch: 2, Training Loss: 0.4831085730493678, Training Accuracy = 0.7787001836191068\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # print(name, param.data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.13806517349902345, Training Accuracy = 0.9525305816329169\n",
      "Epoch: 1, Training Loss: 0.07643346815442473, Training Accuracy = 0.9742286704424962\n",
      "Epoch: 2, Training Loss: 0.051076244030279334, Training Accuracy = 0.983862208084413\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../models/bert-small-vulnerability_scope-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.05796181711718364, Test Accuracy = 0.9836557360091032\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "num_correct = 0 \n",
    "num_examples = 0\n",
    "test_loss = 0\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['encoded_labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    test_loss += loss.data.item() * input_ids.size(0)\n",
    "    correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "    num_correct += torch.sum(correct).item()\n",
    "    num_examples += correct.shape[0]\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "print('Test Loss: {}, Test Accuracy = {}'.format(test_loss, num_correct / num_examples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
