{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64480, 1)\n",
      "(16120, 1)\n",
      "(64480, 1)\n",
      "(16120, 1)\n",
      "cvssv3_user_interaction\n",
      "REQUIRED    0.500062\n",
      "NONE        0.499938\n",
      "Name: count, dtype: float64\n",
      "cvssv3_user_interaction\n",
      "NONE        0.500248\n",
      "REQUIRED    0.499752\n",
      "Name: count, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64480 entries, 0 to 64479\n",
      "Data columns (total 1 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   english_description  64480 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 503.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64480 entries, 0 to 64479\n",
      "Data columns (total 1 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   cvssv3_user_interaction  64480 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 503.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_train = pd.read_csv('./cvss_2022_2024_X_train.csv')\n",
    "y_train = pd.read_csv('./cvss_2022_2024_y_train.csv')\n",
    "\n",
    "X_test = pd.read_csv('./cvss_2022_2024_X_test.csv')\n",
    "y_test = pd.read_csv('./cvss_2022_2024_y_test.csv')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_train['cvssv3_user_interaction'].value_counts(dropna=False) / y_train.shape[0])\n",
    "print(y_test['cvssv3_user_interaction'].value_counts(dropna=False) / y_test.shape[0])\n",
    "print(X_train.info())\n",
    "print(y_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NONE' 'REQUIRED']\n",
      "2\n",
      "0        NONE\n",
      "1        NONE\n",
      "2        NONE\n",
      "3        NONE\n",
      "4    REQUIRED\n",
      "5        NONE\n",
      "6    REQUIRED\n",
      "7        NONE\n",
      "8    REQUIRED\n",
      "9        NONE\n",
      "Name: cvssv3_user_interaction, dtype: object [0 0 0 0 1 0 1 0 1 0]\n",
      "64480 64480 16120 16120\n"
     ]
    }
   ],
   "source": [
    "label_column_name = \"cvssv3_user_interaction\"\n",
    "train_labels = y_train.loc[:, label_column_name]\n",
    "test_labels = y_test.loc[:, label_column_name]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "print(le.classes_)\n",
    "\n",
    "with open(\"../labels/user_interaction_label.txt\", \"wb\") as f:\n",
    "    pickle.dump(le.classes_, f)\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "encoded_train_labels = le.transform(train_labels)\n",
    "encoded_test_labels = le.transform(test_labels)\n",
    "\n",
    "print(train_labels[:10], encoded_train_labels[:10])\n",
    "print(len(X_train), len(train_labels), len(X_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.loc[:,\"english_description\"].tolist(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CVEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, encodings, labels, encoded_labels):\n",
    "        self.texts = X.loc[:,\"english_description\"].tolist()\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "        self.encoded_labels = encoded_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['text_labels'] = self.labels[idx]\n",
    "        item['encoded_labels'] = torch.tensor(self.encoded_labels[idx])\n",
    "        item['vulnerability_description'] = self.texts[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CVEDataset(X_train, train_encodings, train_labels, encoded_train_labels)\n",
    "test_dataset = CVEDataset(X_test, test_encodings, test_labels, encoded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1000,  1037, 18130,  1999,  1037,  3563,  2717, 17928,  2203,\n",
       "          8400,  1997, 26408,  1050, 20952,  2278,  2071,  3499,  2019, 14469,\n",
       "          4383,  1010,  2659,  1011, 21598,  1010,  6556, 17346,  2000,  2039,\n",
       "         11066,  2030,  3972, 12870,  6764,  2006,  2019,  5360,  5080,  1012,\n",
       "          2023, 18130,  6526,  2138,  1997,  4394, 20104,  7711,  2006,  1996,\n",
       "          5360,  2717, 17928,  2203,  8400,  1012,  2019, 17346,  2071, 18077,\n",
       "          2023, 18130,  2011,  6016, 19275, 17928, 11186,  2000,  1996,  5360,\n",
       "          2203,  8400,  1012,  1037,  3144, 18077,  2071,  3499,  1996, 17346,\n",
       "          2000,  2039, 11066,  6764,  2046,  1037,  3563, 11661,  2030,  3972,\n",
       "         12870,  6764,  2013,  1037,  3563, 19622,  2306,  2008, 11661,  1012,\n",
       "          2023, 18130,  2069, 13531,  1037,  3563,  2717, 17928,  2203,  8400,\n",
       "          1998,  2515,  2025,  7461,  1996,  4773,  1011,  2241,  2968,  8278,\n",
       "          1012,  1000,   102,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0]),\n",
       " 'text_labels': 'NONE',\n",
       " 'encoded_labels': tensor(0),\n",
       " 'vulnerability_description': '\"A vulnerability in a specific REST API endpoint of Cisco NDFC could allow an authenticated, low-privileged, remote attacker to upload or delete files on an affected device.    This vulnerability exists because of missing authorization controls on the affected REST API endpoint. An attacker could exploit this vulnerability by sending crafted API requests to the affected endpoint. A successful exploit could allow the attacker to upload files into a specific container or delete files from a specific folder within that container. This vulnerability only affects a specific REST API endpoint and does not affect the web-based management interface.\"'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-small', num_labels=NUM_CLASSES)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight tensor([[-0.0154, -0.0156, -0.0241,  ...,  0.0187,  0.0140,  0.0049],\n",
      "        [-0.0331, -0.0036, -0.0097,  ...,  0.0022, -0.0153, -0.0055]])\n",
      "classifier.bias tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\my-proj\\ML - Proje\\CVSS-Prediction-BERT_small\\venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.6187873020953043, Training Accuracy = 0.6693548387096774\n",
      "Epoch: 1, Training Loss: 0.553558181289112, Training Accuracy = 0.7322425558312655\n",
      "Epoch: 2, Training Loss: 0.5318519908426418, Training Accuracy = 0.7427109181141439\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # print(name, param.data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.1926736428873803, Training Accuracy = 0.9272177419354839\n",
      "Epoch: 1, Training Loss: 0.12570519099692598, Training Accuracy = 0.9550868486352357\n",
      "Epoch: 2, Training Loss: 0.09731902825969928, Training Accuracy = 0.9656017369727047\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_loss_batch = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['encoded_labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        training_loss_batch.append(loss.data.item())\n",
    "        training_loss += loss.data.item() * input_ids.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    training_loss /= len(train_loader.dataset)\n",
    "    training_loss_epoch.append(training_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Training Accuracy = {}'.format(epoch, training_loss, num_correct / num_examples))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../models/bert-small-vulnerability_user_interaction-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.11094766856187367, Test Accuracy = 0.9637096774193549\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "num_correct = 0 \n",
    "num_examples = 0\n",
    "test_loss = 0\n",
    "for batch in test_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['encoded_labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    test_loss += loss.data.item() * input_ids.size(0)\n",
    "    correct = torch.eq(torch.max(F.softmax(outputs.logits, dim=1), dim=1)[1], labels)\n",
    "    num_correct += torch.sum(correct).item()\n",
    "    num_examples += correct.shape[0]\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        \n",
    "print('Test Loss: {}, Test Accuracy = {}'.format(test_loss, num_correct / num_examples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
